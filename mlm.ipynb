{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "\n",
    "from dataloader import get_dataloader\n",
    "from model import BERT_MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 128\n",
    "device = 'cuda'\n",
    "n_layers = 2\n",
    "\n",
    "model = BERT_MLM(d_model, 2*d_model, tokenizer_path=\"data/datasets_full/tokenizer.json\", dropout=0.2, device=device)\n",
    "#model.load_state_dict(torch.load(\"saved_models/2106_1/2106_1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_dataloader(batch_size=128, drop_last=True, mode=\"train\", device=device, mlm_index=\"all\", json_path=\"data/datasets_full\", tokenizer_path=\"data/datasets_full/tokenizer.json\")\n",
    "valid_dl = get_dataloader(batch_size=128, drop_last=True, mode=\"valid\", device=device, mlm_index=\"all\", json_path=\"data/datasets_full\", tokenizer_path=\"data/datasets_full/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a2d83b0f23443091e1f2b51bbb7e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train: loss 3.86; accuracy 0.29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3cf720fa2e4bf5baa729fed6c4b5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: valid: loss 3.12; accuracy 0.41\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4\n",
    "    )\n",
    "\n",
    "epochs = 1\n",
    "n_prev_epochs = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch = epoch + n_prev_epochs\n",
    "    \n",
    "    model.train(); torch.cuda.empty_cache()\n",
    "    epoch_loss, epoch_acc = [], []\n",
    "\n",
    "    for batch in tqdm(train_dl):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tokens = batch[\"tokens\"]\n",
    "        targets = batch[\"target_tokens\"]\n",
    "        \n",
    "        predicted_tokens = model(tokens)\n",
    "\n",
    "        was_mlm_token = tokens == 2\n",
    "        mlm_predicted_tokens = predicted_tokens[was_mlm_token]\n",
    "\n",
    "        loss = criterion(mlm_predicted_tokens, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.detach().item())\n",
    "        epoch_acc.append((targets == mlm_predicted_tokens.argmax(dim=1)).float().mean().item())\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_acc = np.array(epoch_acc).mean()\n",
    "\n",
    "    print(f\"Epoch {1+epoch}:\", \"train: loss {:.2f}; accuracy {:.2f}\".format(epoch_loss, epoch_acc))\n",
    "\n",
    "    model.eval(); torch.cuda.empty_cache()\n",
    "    epoch_loss, epoch_acc = [], []\n",
    "\n",
    "    for batch in tqdm(valid_dl):\n",
    "        tokens = batch[\"tokens\"]\n",
    "        targets = batch[\"target_tokens\"]\n",
    "        \n",
    "        predicted_tokens = model(tokens)\n",
    "\n",
    "        was_mlm_token = tokens == 2\n",
    "        mlm_predicted_tokens = predicted_tokens[was_mlm_token]\n",
    "\n",
    "        loss = criterion(mlm_predicted_tokens, targets)\n",
    "        \n",
    "        epoch_loss.append(loss.detach().item())\n",
    "        epoch_acc.append((targets == mlm_predicted_tokens.argmax(dim=1)).float().mean().item())\n",
    "        \n",
    "        break\n",
    "\n",
    "    epoch_loss = np.array(epoch_loss).mean()\n",
    "    epoch_acc = np.array(epoch_acc).mean()\n",
    "\n",
    "    print(f\"Epoch {1+epoch}:\", \"valid: loss {:.2f}; accuracy {:.2f}\".format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), \"saved_models/2006_1/2006_1.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul des AUPRC AUROC pour les 3 classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = get_dataloader(batch_size=128, drop_last=True, shuffle=True, mode=\"test\", device=\"cpu\", mlm_index=\"labels\", json_path=\"data/datasets_full\", tokenizer=tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score as AUPRC\n",
    "from sklearn.metrics import roc_auc_score as AUROC\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "classes = [\"dies\", \"readm\", \"duration\"]\n",
    "binary_mapper = {\n",
    "    \"dies\": [268, 27], \n",
    "    \"readm\": [94, 33], \n",
    "    \"duration\": [110, 32]\n",
    "    }\n",
    "\n",
    "labels, binary_probas = {cls:[] for cls in classes}, {cls:[] for cls in classes}\n",
    "\n",
    "for batch in tqdm(test_dl):\n",
    "    tokens = batch[\"tokens\"].to(device)\n",
    "    targets = batch[\"target_tokens\"].to(device)\n",
    "    \n",
    "    predicted_logits = model(tokens)\n",
    "\n",
    "    was_mlm_index = tokens == 2\n",
    "    mlm_predicted_logits = predicted_logits[was_mlm_index]\n",
    "    predicted_tokens = mlm_predicted_logits.argmax(dim=1)\n",
    "\n",
    "    predicted_tokens = predicted_tokens.reshape((-1, 3)).to('cpu').detach().numpy()\n",
    "    targets = targets.reshape((-1, 3)).to('cpu').detach().numpy()\n",
    "\n",
    "    for idx, inference_type in enumerate(classes):\n",
    "        y_true = targets[:, idx]\n",
    "\n",
    "        one, zero = binary_mapper[inference_type]\n",
    "\n",
    "        slicer = torch.tensor([zero, one])\n",
    "        logits_of_interest = mlm_predicted_logits[idx::len(classes), slicer]\n",
    "        probas_of_interest = torch.nn.Softmax(dim=-1)(logits_of_interest).to('cpu').detach().numpy()\n",
    "        probas_of_interest = probas_of_interest[:, 1]\n",
    "\n",
    "        y_true_binary = np.where(y_true == one, np.ones_like(y_true), np.zeros_like(y_true))\n",
    "\n",
    "        labels[inference_type].append(y_true_binary)\n",
    "        binary_probas[inference_type].append(probas_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {k:np.concatenate(v) for k,v in labels.items()}\n",
    "binary_probas = {k:np.concatenate(v) for k,v in binary_probas.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROC(labels[\"duration\"], binary_probas[\"duration\"]), AUROC(labels[\"readm\"], binary_probas[\"readm\"]), AUROC(labels[\"dies\"], binary_probas[\"dies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUPRC(labels[\"duration\"], binary_probas[\"duration\"]), AUPRC(labels[\"readm\"], binary_probas[\"readm\"]), AUPRC(labels[\"dies\"], binary_probas[\"dies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unige",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
